{
  "PERSONAL_EMAIL": "jacquelineliu1997@gmail.com",
  "PERSONAL_FULL_NAME": "Qin Liu",
  "PERSONAL_LAST_NAME": "Liu",
  "PERSONAL_FIRST_NAME": "Qin",
  "PERSONAL_COUNTRY_ORIGIN": "China",
  "PERSONAL_ADDRESS_LINE1": "1710 R ST",
  "PERSONAL_CITY": "Sacramento",
  "PERSONAL_STATE": "CA",
  "PERSONAL_ZIP": "95811",
  "PERSONAL_PHONE": "2137058435",
  "LETTER_DATE": "July 20, 2025",
  "USCIS_ADDRESS_LINE1": "2501 S. State Hwy. 121 Business",
  "USCIS_CITY": "Lewisville",
  "USCIS_STATE": "TX",
  "USCIS_ZIP": "75067-8003",
  "HIS_HER_THEIR": "her",
  "HE_SHE_THEY": "she",
  "HIM_HER_THEM": "her",
  "HIMSELF_HERSELF_THEMSELF": "herself",
  "EDU_DEGREE_1": "B.A.",
  "EDU_MAJOR_1": "Philosophy",
  "EDU_UNIVERSITY_1": "Fudan University",
  "EDU_GOVERNING_BODY": "the Ministry of Education",
  "EDU_RANKING_YEAR": "2026",
  "EDU_RANK_GLOBAL": "30",
  "EDU_RANK_NATIONAL": "3",
  "EDU_SUBFIELD": "philosophy",
  "EDU_RANKING_SOURCE": "QS",
  "EDU_SUBFIELD_RANK": "41",
  "EDU_FIELD_OF_STUDY": "philosophy",
  "EDU_NATIONAL_RANK": "5",
  "EDU_COUNTRY": "China",
  "EDU_DEGREE_2": "M.S.",
  "EDU_MAJOR_2": "Computer Science",
  "EDU_UNIVERSITY_2": "Fudan University",
  "EDU_YEAR_2": "2022",
  "EDU_TYPE_2": "comprehensive public research university",
  "EDU_SIZE_RANK_2": "4",
  "EDU_LOCATION_2": "Shanghai, China",
  "EDU_YEAR": "third-year PhD",
  "EDU_BEST_VALUE_RANK": "N/A",
  "EDU_KEY_RESEARCH_AREAS": "machine learning, artificial intelligence, AI Safety & LLM Reliability, Trustworthy and Ethical AI",
  "EDU_STATE": "N/A",
  "EDU_FIELD": null,
  "CURRENT_POSITION": "PhD student",
  "CURRENT_COMPANY": "University of California, Davis",
  "CURRENT_LOCATION": "Davis, California",
  "COMPANY_FOUNDING_YEAR": "1905",
  "COMPANY_INDUSTRY": "education",
  "COMPANY_SERVICES": "UC Davis provides nationally important services in health, agriculture, and education, with growing national impact in computer science and artificial intelligence. The university supports cutting-edge research in AI safety, machine learning, natural language processing, and cybersecurity through its Computer Science Department and interdisciplinary institutes. Faculty and PhD students actively contribute to federally funded research on trustworthy AI, human-centered computing, and responsible data science, helping shape national policy and safety standards. UC Davis also plays a key role in training the next generation of computer scientists through NSF-funded programs, AI4ALL outreach, and undergraduate research initiatives that broaden participation in STEM. These efforts complement its nationally recognized work in public health (e.g., the UC Davis Medical Center and MIND Institute), global disease surveillance (One Health Institute), and sustainable agriculture (UC ANR). Together, UC Davis\u2019s contributions in computing and beyond advance critical national priorities in technology, safety, public health, and education.",
  "COMPANY_REGIONS": "UC Davis operates across multiple regions in California, extending its impact far beyond its main campus. The central campus in Davis serves as the academic and research hub, housing core programs in computer science, engineering, and life sciences. In nearby Sacramento, the UC Davis Medical Center provides advanced clinical care and telemedicine services, serving communities across the state. The university also maintains specialized research facilities in diverse ecological regions, including the Tahoe Environmental Research Center in Lake Tahoe and the Bodega Marine Laboratory on the northern California coast. Its School of Veterinary Medicine extends into Southern California, offering clinical services and outreach. Additionally, through the UC Agriculture and Natural Resources (UC ANR) division, UC Davis supports Cooperative Extension offices and agricultural research centers across nearly every California county. These distributed operations allow UC Davis to contribute statewide to public health, sustainable agriculture, environmental science, and advanced computing research.",
  "COMPANY_SOLUTIONS": "UC Davis offers nationally impactful solutions in computer science, particularly in the areas of artificial intelligence, natural language processing, cybersecurity, and responsible computing. Faculty and research labs contribute cutting-edge work on AI safety, large language model (LLM) reliability, and human-centered machine learning\u2014addressing urgent national priorities related to trustworthy and ethical AI deployment. UC Davis researchers also develop tools for robust NLP systems, algorithmic fairness, privacy-preserving computation, and secure software infrastructure, many of which are supported by federal agencies such as the NSF, NIH, and DARPA.  The university plays a leadership role in advancing national research agendas through interdisciplinary centers and collaborations that connect computer science with public health, law, and environmental science. UC Davis also supports workforce development through its graduate and undergraduate CS programs, and broadens national participation in computing through initiatives like AI4ALL and inclusive CS education partnerships. These efforts collectively ensure that UC Davis is not only advancing the frontier of computing research, but also shaping how AI and digital technologies are safely and equitably integrated into society at a national scale.",
  "EMPLOYEE_COUNT": "24,000",
  "REVENUE": "$7.3 billion",
  "INDUSTRY": null,
  "YEARS_OF_EXPERIENCE": "0, still in PhD program",
  "RESEARCH_FIELD": "computer science and artificial intelligence",
  "INDUSTRY_SECTOR": "Information Technology",
  "FIELD_PURPOSE": "The primary purpose of my field\u2014artificial intelligence and trustworthy natural language processing\u2014is to develop safe, reliable, and transparent AI systems that serve the public good and support national priorities. As large language models and AI technologies are increasingly integrated into sectors like healthcare, education, law, defense, and public infrastructure, ensuring their safety, robustness, and alignment with human values is critical. My work focuses on advancing methods to detect and mitigate harmful or unintended AI behaviors, improve model interpretability, and establish standards for responsible deployment. This contributes to national goals by enhancing the security, ethical use, and trustworthiness of AI systems, while supporting innovation and maintaining U.S. leadership in emerging technologies.",
  "TECHNICAL_REQUIREMENTS": null,
  "FIELD_CHALLENGES": "To address the core challenges in trustworthy AI and NLP\u2014particularly those your research tackles\u2014the field must advance in several key directions:  **1. Strengthen Alignment and Robustness Techniques** Large language models often exhibit unsafe or unintended behaviors under adversarial or ambiguous prompts. Addressing this requires scalable alignment strategies such as reward modeling, denoised product-of-experts, and multi-turn red teaming, all of which are central themes in your work.  **2. Enhance Evaluation Beyond Standard Benchmarks** Traditional accuracy metrics fail to capture risks related to misuse, hallucination, or misalignment. The field needs fine-grained, safety-focused evaluation protocols\u2014including behavioral auditing, context-sensitive testing, and attack success rate analysis\u2014which your research actively advances.  **3. Improve Resilience Against Jailbreaks and Unsafe Prompting** LLMs remain vulnerable to prompt-based exploits. Tackling this requires deeper understanding of representation-level vulnerabilities and robust, model-internal safety filters. Your work contributes by diagnosing and mitigating such failures through backdoor defense and probing mechanisms.  **4. Design Scalable Oversight and Monitoring Systems** As models grow in size and capability, continuous post-deployment oversight becomes essential. This includes real-time output monitoring, interpretability tools, and methods for detecting emergent unsafe behaviors\u2014areas where your framework-level contributions are especially relevant.  **5. Build Foundations for Human-AI Trust and Governance** To ensure responsible integration of AI into sensitive domains, the field must integrate human-in-the-loop systems, enforceable alignment guarantees, and policy-aware design principles. Your research helps define how LLMs can be steered and evaluated in complex, real-world settings.  Together, these directions reflect not only the field\u2019s priorities but also the concrete problems my publications seek to address\u2014advancing the reliability, safety, and societal alignment of next-generation language technologies.",
  "NEGATIVE_CONSEQUENCES": "If the technical challenges in trustworthy AI and natural language processing are not addressed, the nation could face several serious consequences:  **1. Misinformation and Social Harm** Unchecked large language models may generate convincing false information, amplifying disinformation campaigns, eroding public trust, and destabilizing democratic discourse.  **2. Safety Failures in High-Stakes Domains** AI systems deployed in healthcare, law, defense, or education may produce unreliable or biased outputs, leading to harmful decisions, legal liabilities, and loss of human life or civil rights.  **3. Exploitation via Jailbreaks and Adversarial Attacks** Without robust safeguards, malicious actors could exploit LLMs to produce harmful content, generate malware, or leak sensitive information\u2014posing cybersecurity and national security threats.  **4. Algorithmic Discrimination and Societal Inequity** Failure to address bias and fairness issues in NLP models may perpetuate systemic discrimination, disproportionately affecting marginalized groups in areas like hiring, lending, or law enforcement.  **5. Erosion of Public Trust and Technological Backlash** If models behave unpredictably or cause harm, public confidence in AI may collapse, leading to widespread resistance, stalled innovation, and regulatory overcorrection that slows national progress in AI research.  These risks underscore the urgent need for research that builds safe, interpretable, and aligned AI systems capable of supporting national interests responsibly and reliably.",
  "PRIMARY_RESEARCH_AREA": null,
  "TECHNICAL_METHODS": null,
  "RESEARCH_OBJECTIVES": "the safety, reliability, transparency, and robustness of large language models and AI systems",
  "PRACTICAL_APPLICATIONS": "develop AI responsibly, safeguard against misuse, support high-stakes decision-making, and inform technology policy",
  "RESEARCH_FOCUS": "\u202fsafety alignment and failure modes in large language model systems",
  "TECHNICAL_PROBLEM": "large language models can exhibit unsafe or misaligned behavior under benign or adversarial inputs",
  "TECHNICAL_CHALLENGE": "emergent unsafe behaviors that are difficult to predict, reproduce, or formally verify",
  "KEY_METRIC": "AI systems' safeness and robustness against adversarial user inputs",
  "INDUSTRY_IMPACT": "the safe and reliable deployment of AI systems in high-stakes national domains such as healthcare, education, public policy, and security",
  "RESEARCH_SUBJECT": "the behavior and reliability of large language models",
  "DATA_TYPES": "model outputs, internal representations, failure cases, and adversarial user inputs.",
  "CORE_METHODOLOGY": "training-time defenses, inference-time interventions, and representation-level manipulations",
  "METHODOLOGY_PURPOSE": "diagnosing, mitigating, and preventing unsafe or misaligned behaviors in large language models and AI systems",
  "ANALYSIS_OUTPUT": "alignment risk profiles, adversarial vulnerabilities, and behavioral failure patterns",
  "TECHNICAL_LIMITATIONS": "the lack of reliable methods to anticipate and generalize model behavior across diverse real-world contexts",
  "ADVERSE_EFFECTS": "undetected safety failures, poor robustness under distribution shifts, and vulnerability to adversarial or manipulative inputs in high-stakes deployments",
  "NOVEL_SOLUTION": "LLM knowledge access control framework named SudoLM",
  "SOLUTION_BENEFITS": "regulates internal knowledge activation in large language models to preserve safety alignment and prevent unauthorized behavior.",
  "SECONDARY_SOLUTION": "a denoised product-of-experts (PoE) defense framework based on ensembled training",
  "PROBLEM_RESOLUTION": "mitigate backdoor attacks in large language models by suppressing malicious triggers while preserving benign task performance.",
  "DATA_QUALITY": "robust filtering of adversarial or misaligned data signals during training and evaluation",
  "RESEARCH_PERFORMANCE_METRIC": "model alignment, safety consistency, and resistance to backdoor or prompt-based attacks.",
  "OPTIMIZATION_APPROACH": "ensembles-based training optimization framework",
  "SPECIFIC_APPLICATION": "improving the robustness and safety alignment of large language models",
  "APPLICATION_SIGNIFICANCE": " deploying AI systems in nationally sensitive domains such as healthcare, law, and defense where reliability and trust are paramount",
  "TECHNICAL_STRATEGY": "authorization-aligned model knowledge access control strategy",
  "TECHNICAL_CHALLENGES": "unauthorized knowledge activation and misaligned behavior in large language models",
  "POSITIVE_OUTCOMES": "safer, more controllable AI systems suitable for deployment in critical national domains such as healthcare, law, and public infrastructure.",
  "ANALYSIS_FOCUS": "LLM knowledge access control",
  "METHODOLOGY_INTEGRATION": "authorization-aligned policy learning with key-based routing and inference-time supervision to ensure controlled and modular knowledge activation in large language models.",
  "SPECIFIC_TECHNIQUE": "Direct Preference Optimization (DPO)",
  "DATA_SOURCES": "synthetic and real-world datasets",
  "ERROR_TYPES": "misalignment errors and unauthorized knowledge activation",
  "APPLICATION_DOMAIN": "large language model instruction following and access control.",
  "RELATED_DISCIPLINE": "AI policy and governance",
  "TECHNICAL_METHOD": "authorization-based Direct Preference Optimization",
  "METHOD_PURPOSE": "ensuring that language models respond in ways aligned with authorized user intent, thereby supporting safe and responsible AI deployment in nationally critical sectors",
  "KEY_RESULTS": "improved alignment consistency, reduced unauthorized behavior, and enhanced control over language model responses",
  "IMPROVEMENT_AREA": "safer AI deployment, increased trustworthiness in high-stakes applications, and stronger safeguards against misuse",
  "LONG_TERM_OBJECTIVE": "the safe, controllable, and responsible deployment of large language models in service of national interests across healthcare, education, public policy, and security.",
  "SECONDARY_FIELD": "Vision\u2013Language Model safety",
  "FIELD_CHALLENGE": "the challenge of safety degradation caused by modality shifts and cross-modal interactions that can trigger unsafe or misaligned responses in multi-modal AI systems.",
  "INNOVATIVE_SYSTEM": "inference-time representation manipulation system (CMRM)",
  "AUTOMATION_TARGET": "restoring safety alignment in vision\u2013language models without requiring additional training",
  "APPLICATION_AREA": "ensuring the safety and alignment of large language and vision\u2013language models",
  "VALIDATION_DATA_TYPE_1": "cross-modal adversarial image\u2013text pairs",
  "VALIDATION_DATA_TYPE_2": "vision\u2013language safety benchmarks",
  "PERFORMANCE_METRIC_1": "unsafe response rate reduction",
  "PERFORMANCE_METRIC_2": "alignment preservation under modality shifts",
  "PERFORMANCE_METRIC_3": "robustness against adversarial image\u2013text inputs",
  "INNOVATIVE_TECHNOLOGIES": "alignment-aware large language models and vision\u2013language intervention techniques",
  "BENEFIT_1": "safer",
  "BENEFIT_2": "more reliable",
  "BENEFIT_3": "resilient to misuse in high-stakes national applications",
  "PAPER_TITLE_1": "Mantis: Interleaved Multi-Image Instruction Tuning",
  "JOURNAL_1": "Transactions on Machine Learning Research",
  "INSTITUTION_1": "University of Waterloo, Tsinghua University, Sea AI Lab",
  "QUOTED_COMMENTARY_1": "MuirBench (Wang et al., 2024a) is a comprehensive benchmark consisting of 12 diverse multi-image tasks, such as scene understanding, ordering, etc. It contains 2,600 multiple-choice questions with 11,264 images involved in total. We report the overall average performance across the 12 tasks.",
  "PAPER_TITLE_2": "Defending LVLMs Against Vision Attacks Through Partial-Perception Supervision",
  "JOURNAL_2": "International Conference on Machine Learning",
  "INSTITUTION_2": "Zhejiang University, Nanyang Technological University, Shanghai Jiao Tong University, National University of Singapore",
  "QUOTED_COMMENTARY_2": "However, research indicates that LVLMs demonstrate weaker defense performance compared to LLMs (Liu et al., 2024c). ",
  "PAPER_TITLE_3": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking",
  "JOURNAL_3": "Conference on Neural Information Processing Systems",
  "INSTITUTION_3": "Facebook AI, Stanford University",
  "QUOTED_COMMENTARY_3": "That is, following past work on NLP robustness [30, 31], we perturb examples and measure whether a model\u2019s prediction changes. Specifically, we use the recently released TextFlint5 evaluation toolkit [24] to do so (details are provided in Appendix B). ; We thank the TextFlint team for their help on the toolkit. ",
  "PAPER_TITLE_4": "RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks",
  "JOURNAL_4": "Annual Meeting of the Association for Computational Linguistics",
  "INSTITUTION_4": "Sun Yat-sen University, Guangxi University, Guangdong Key Laboratory of Big Data Analysis and Processing, Key Laboratory of Machine Intelligence and Advanced Computing",
  "QUOTED_COMMENTARY_4": "Flooding-X (Liu et al., 2022) improves Flooding (Ishida et al., 2020) to boost model generalization by preventing further reduction of the training loss.; Compared to the state-of-the-art method Flooding-X across all vic- tim models and datasets ; Further, it can achieve performance on par with Flooding-X when enabling the transformation, while only incurring a slight increase in computational overhead.; Flooding-X. We use the original hyperparam- eters setting in their paper (Liu et al., 2022) of BERT model.",
  "PAPER_TITLE_5": "MIND CONTROL THROUGH CAUSAL INFERENCE: PRE- DICTING CLEAN IMAGES FROM POISONED DATA",
  "JOURNAL_5": "International Conference on Learning Representations",
  "INSTITUTION_5": "University of Virginia, Virginia Tech, University of Maryland, College Park, Merck & Co., Inc., University of Georgia",
  "QUOTED_COMMENTARY_5": "(1) The structure is much smaller than SFRN, as backdoor patterns are simpler to learn than normal patterns, as evidenced by (Liu et al., 2023b; Zhang et al., 2023); Firstly, the AIN is intentionally designed to have a much smaller structure than the SFRN, as backdoor patterns are simpler and quicker to learn than normal patterns, as evidenced by prior work (Liu et al., 2023b; Zhang et al., 2023; Yu et al., 2022; Sandoval-Segura et al., 2022).",
  "PUBLICATIONS_TOTAL_COUNT": "26",
  "JOURNAL_1_NAME": "MUIRBENCH: A COMPREHENSIVE BENCHMARK FOR ROBUST MULTI-IMAGE UNDERSTANDING",
  "JOURNAL_1_FIELD": "Computer Vision and Multimodal Machine Learning",
  "JOURNAL_1_SCOPE": "Developing systematic evaluation frameworks for assessing the robustness, generalization, and reasoning capabilities of AI models in multi-image understanding tasks.",
  "JOURNAL_1_YEAR": "2025",
  "JOURNAL_1_IMPACT_FACTOR": "304",
  "JOURNAL_2_NAME": "TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing",
  "JOURNAL_2_FIELD": "natural language processing",
  "JOURNAL_2_SCOPE": "Natural Language Processing robustness evaluation and toolkit development.",
  "JOURNAL_2_YEAR": "2025",
  "JOURNAL_2_IMPACT_FACTOR": "215",
  "JOURNAL_3_NAME": "Flooding-X: Improving BERT\u2019s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning",
  "JOURNAL_3_FIELD": "Natural Language Processing (NLP) robustness",
  "JOURNAL_3_SCOPE": "Enhancing the adversarial robustness of BERT by introducing a training-time regularization method (Flooding-X) that improves resistance to textual adversarial attacks without added computational cost.",
  "JOURNAL_3_YEAR": "2015",
  "JOURNAL_3_IMPACT_FACTOR": "215",
  "JOURNAL_4_NAME": "From Shortcuts to Triggers: Backdoor Defense with Denoised PoE",
  "JOURNAL_4_FIELD": " NLP model robustness",
  "JOURNAL_4_SCOPE": "defending against adversarial backdoor attacks using a novel denoised product-of-experts approach",
  "JOURNAL_4_FOCUS": "Mitigating backdoor vulnerabilities in NLP models by separating malicious trigger learning from clean task learning through a denoised product-of-experts framework.",
  "JOURNAL_5_NAME": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
  "JOURNAL_5_FIELD": "Natural Language Processing robustness and prompt engineering",
  "JOURNAL_5_SCOPE": "Developing and evaluating an end-to-end decoding strategy that systematically rewrites prompts into lower-perplexity paraphrases\u2014without additional training\u2014to enhance zero-shot generalization on unseen tasks and instructions.",
  "CITATIONS_SOURCE": "Google Scholar",
  "CITATIONS_TOTAL_COUNT": "634",
  "COUNTRY_1": "United States",
  "COUNTRY_2": "China",
  "COUNTRY_3": "United Kingdom",
  "COUNTRY_4": "Singapore",
  "COUNTRY_5": "Australia",
  "INSTITUTION_1_CITING": "Fudan University",
  "INSTITUTION_2_CITING": "University of California, Davis",
  "INSTITUTION_3_CITING": "Nanyang Technological University",
  "INSTITUTION_4_CITING": "Carnegie Mellon University",
  "INSTITUTION_5_CITING": "Stanford University",
  "COMPANY_1": "Google",
  "COMPANY_2": "Microsoft",
  "COMPANY_3": "Meta",
  "COMPANY_4": "Apple",
  "COMPANY_5": "NVIDIA",
  "PUBLICATION_LIST": "1. Secrets of rlhf in large language models part i: Ppo; 2. Textflint: Unified multilingual robustness evaluation toolkit for natural language processing; 3. Muirbench: A comprehensive benchmark for robust multi-image understanding; 4. Flooding-X: Improving BERT\u2019s resistance to adversarial attacks via loss-restricted fine-tuning; 5. From shortcuts to triggers: Backdoor defense with denoised poe; 6. Test-time backdoor mitigation for black-box large language models with defensive demonstrations; 7. Securing multi-turn conversational language models from distributed backdoor attacks; 8. Unraveling and mitigating safety alignment degradation of vision-language models; 9. Monotonic paraphrasing improves generalization of language model prompting; 10. Metascale: Test-time scaling with evolving meta-thoughts; 11. Sudolm: Learning access control of parametric knowledge with authorization alignment",
  "FIELD_LIST": "artificial intelligence, natural language processing, machine learning security, and vision\u2013language modeling",
  "TECHNIQUE_LIST": "preference optimization, adversarial evaluation, backdoor defense, prompt-based alignment, model knowledge access control, and zero-shot generalization",
  "KEY_DISCIPLINES_LIST": "Artificial Intelligence, Natural Language Processing, Machine Learning Security, and Trustworthy Computing",
  "FIELD_COMBINATION": "machine learning, natural language processing, adversarial robustness, and AI policy and governance.",
  "APPLICATION_AREAS": "AI safety and security, public policy support, healthcare decision-making, education technology, and national defense systems.",
  "NATIONAL_PRIORITY_AREAS": "secure AI deployment, innovation leadership, and the safe integration of advanced technologies into essential public services.",
  "GLOBAL_CHALLENGES": "AI misuse and disinformation, technological safety and alignment, digital trust and security, and the governance of advanced AI systems.",
  "CORE_RESEARCH_AREAS": "machine learning, natural language processing, AI safety,  trustworthy AI systems",
  "RESEARCH_FIELD_LIST": "artificial intelligence, natural language processing, machine learning, vision\u2013language modeling, and trustworthy computing",
  "RESEARCH_FOCUS_LIST": "preference optimization, adversarial robustness, backdoor defense, prompt-based alignment, access control in language models, zero-shot generalization, and vision\u2013language model safety.",
  "SPECIALIZED_FIELD": "artificial intelligence, AI safety",
  "SPECIALIZED_SKILLS": "preference optimization, adversarial evaluation, backdoor defense, prompt optimization, safety alignment, model knowledge access control, and multi-modal model auditing.",
  "ADVANCED_METHODOLOGIES": "machine learning, deep learning, preference optimization",
  "INDUSTRY_CHALLENGE": "ensuring the safety, realibility, and robustness of large language and vision\u2013language models in real-world, high-stakes deployments.",
  "METHOD_1": "deep learning",
  "METHOD_2": "alignment-aware optimization",
  "SKILL_LIST": "preference optimization, adversarial robustness, backdoor defense, prompt optimization, alignment tuning, access control in language models, red teaming, and vision\u2013language model optimization",
  "INDUSTRY_APPLICATION": "artificial intelligence and national security industries."
}